{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31227732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                  \n",
    "import torch.nn as nn                          \n",
    "import torch.nn.functional as F  \n",
    "\n",
    "import numpy as np      \n",
    "\n",
    "import time\n",
    "import flappy_bird_gym\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3c777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32                                 \n",
    "LR = 0.001                                       \n",
    "EPSILON = 0.999                \n",
    "GAMMA = 0.9                                     \n",
    "TARGET_REPLACE_ITER = 100                       \n",
    "MEMORY_CAPACITY = 1000\n",
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")         \n",
    "N_ACTIONS = env.action_space.n                 \n",
    "N_STATES = env.observation_space.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5050291",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_hidden):                                                        \n",
    "       \n",
    "        super(Net, self).__init__()                                             \n",
    "        self.fc1 = nn.Linear(n_states, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.fc3 = nn.Linear(n_hidden, n_hidden)\n",
    "        #self.fc4 = nn.Linear(n_hidden, n_hidden)\n",
    "        #self.fc5 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(n_hidden, n_actions)                                     \n",
    "        \n",
    "    def forward(self, x):                                                       \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #x = F.relu(self.fc4(x))\n",
    "        #x = F.relu(self.fc5(x))\n",
    "        actions_value = self.out(x)                                           \n",
    "        return actions_value       \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8863eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, n_states, n_actions, n_hidden, \n",
    "                 batch_size, lr, epsilon, gamma, target_replace_iter, memory_capacity): \n",
    "                                                             \n",
    "        self.eval_net = Net(n_states, n_actions, n_hidden)\n",
    "        self.target_net = Net(n_states, n_actions, n_hidden)\n",
    "        \n",
    "        self.learn_step_counter = 0                                             \n",
    "        self.memory_counter = 0\n",
    "        \n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    \n",
    "        self.loss_func = nn.MSELoss()\n",
    "        \n",
    "        self.loss_record=[]\n",
    "\n",
    "    def choose_action(self, x):                                                 \n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)                            \n",
    "        if np.random.uniform() < EPSILON:                                       \n",
    "            actions_value = self.eval_net.forward(x)                            \n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()                \n",
    "            action = action[0]                                                  \n",
    "        else:                                                                  \n",
    "            action = np.random.randint(0, N_ACTIONS)                        \n",
    "        return action                                                           \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):                                    \n",
    "        transition = np.hstack((state, [action, reward], next_state))                                \n",
    "\n",
    "        index = self.memory_counter % MEMORY_CAPACITY                           \n",
    "        self.memory[index, :] = transition                                      \n",
    "        self.memory_counter =self.memory_counter + 1                                                \n",
    "\n",
    "    def learn(self):                                                            \n",
    "        #print(self.memory.shape)\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  \n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())         \n",
    "        self.learn_step_counter = self.learn_step_counter + 1                                            \n",
    "\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)            \n",
    "        \n",
    "        \n",
    "        \n",
    "        b_memory = np.zeros((BATCH_SIZE, N_STATES * 2 + 2))\n",
    "        \n",
    "        N=3\n",
    "        for i in range(BATCH_SIZE):\n",
    "            r=0\n",
    "           \n",
    "            s=self.memory[(sample_index[i]), :N_STATES]\n",
    "            a=self.memory[(sample_index[i]), N_STATES:N_STATES+1]\n",
    "            for j in range(N):\n",
    "                r+=self.memory[(sample_index[i]+j)%MEMORY_CAPACITY, N_STATES+1:N_STATES+2]\n",
    "            s_=self.memory[(sample_index[i]+N)%MEMORY_CAPACITY, -N_STATES:]\n",
    "            t = np.hstack((s, a, r, s_))\n",
    "            b_memory[i, :] = t\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #b_memory = self.memory[sample_index, :]        \n",
    "        \n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        \n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "       \n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "       \n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "\n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)\n",
    "  \n",
    "        q_next = self.target_net(b_s_).detach()\n",
    "  \n",
    "        q_target = b_r + GAMMA * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "    \n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "        \n",
    "        self.loss_record.append(loss)\n",
    "        \n",
    "        self.optimizer.zero_grad()                                      \n",
    "        loss.backward()                                                 \n",
    "        self.optimizer.step()\n",
    "        \n",
    "\n",
    "    def plot_loss(self):\n",
    "        \n",
    "        plt.plot(np.arange(len(self.loss_record)),self.loss_record,'r')\n",
    "        plt.title(\"Loss Record\") # title\n",
    "        plt.ylabel(\"Loss\") # y label\n",
    "        plt.xlabel(\"Step\") # x label\n",
    "        plt.show()\n",
    "        \n",
    "    def save_params(self):\n",
    "        #print(self.eval_net.state_dict())\n",
    "        torch.save(self.eval_net.state_dict(),'params.pkl')\n",
    "    \n",
    "    def load_params(self):\n",
    "        self.eval_net.load_state_dict(torch.load('params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b01ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(N_STATES,N_ACTIONS,128,BATCH_SIZE,LR,EPSILON,GAMMA,TARGET_REPLACE_ITER,MEMORY_CAPACITY)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9211c5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<Episode: 1\n",
      "[0.45486111 0.33765625 0.18765625 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 2\n",
      "[0.45486111 0.58179687 0.43179688 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 3\n",
      "[0.45486111 0.37085938 0.22085938 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 4\n",
      "[0.45486111 0.49390625 0.34390625 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 5\n",
      "[0.45486111 0.37867188 0.22867188 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 6\n",
      "[0.45486111 0.365      0.215      1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 7\n",
      "[0.45486111 0.55445312 0.40445313 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 8\n",
      "[0.45486111 0.58179687 0.43179688 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 9\n",
      "[0.45486111 0.58765625 0.43765625 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 10\n",
      "[0.45486111 0.3728125  0.2228125  1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 11\n",
      "[0.45486111 0.46460938 0.31460938 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 12\n",
      "[0.45486111 0.35132813 0.20132813 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 13\n",
      "[0.45486111 0.45289063 0.30289063 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 14\n",
      "[0.45486111 0.4821875  0.3321875  1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 15\n",
      "[0.45486111 0.45484375 0.30484375 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 16\n",
      "[0.45486111 0.3728125  0.2228125  1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 17\n",
      "[0.45486111 0.53101562 0.38101563 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 18\n",
      "[0.45486111 0.35132813 0.20132813 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 19\n",
      "[0.45486111 0.41382813 0.26382813 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 20\n",
      "[0.45486111 0.4275     0.2775     1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 21\n",
      "[0.45486111 0.38257813 0.23257813 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 22\n",
      "[0.45486111 0.536875   0.386875   1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 23\n",
      "[0.45486111 0.47242188 0.32242188 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 24\n",
      "[0.45486111 0.568125   0.418125   1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 25\n",
      "[0.45486111 0.43726563 0.28726563 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 26\n",
      "[0.45486111 0.47242188 0.32242188 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 27\n",
      "[0.45486111 0.3415625  0.1915625  1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 28\n",
      "[0.45486111 0.46460938 0.31460938 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 29\n",
      "[0.45486111 0.4040625  0.2540625  1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 30\n",
      "[0.45486111 0.49390625 0.34390625 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 31\n",
      "[0.45486111 0.3728125  0.2228125  1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 32\n",
      "[ 0.02430556 -0.28242187 -0.43242188 -0.9       ] -1.9162 {'score': 0}\n",
      "<<<<<<<<<Episode: 33\n",
      "[ 0.         -0.00117188 -0.15117188 -0.7       ] -0.7342843749999999 {'score': 0}\n",
      "<<<<<<<<<Episode: 34\n",
      "[ 0.02430556 -0.1359375  -0.2859375  -0.6       ] -1.354871875 {'score': 0}\n",
      "<<<<<<<<<Episode: 35\n",
      "[ 0.02430556 -0.1984375  -0.3484375  -0.3       ] -1.5429578125 {'score': 0}\n",
      "<<<<<<<<<Episode: 36\n",
      "[ 0.02430556 -0.13398437 -0.28398438 -0.9       ] -1.28573125 {'score': 0}\n",
      "<<<<<<<<<Episode: 37\n",
      "[ 0.02430556 -0.18671875 -0.33671875 -0.5       ] -1.5787 {'score': 0}\n",
      "<<<<<<<<<Episode: 38\n",
      "[ 0.02430556 -0.19257812 -0.34257813 -0.9       ] -1.5716687500000002 {'score': 0}\n",
      "<<<<<<<<<Episode: 39\n",
      "[ 0.02430556 -0.03046875 -0.18046875 -0.9       ] -0.956434375 {'score': 0}\n",
      "<<<<<<<<<Episode: 40\n",
      "[ 0.02430556 -0.05       -0.2         0.3       ] -1.17323125 {'score': 0}\n",
      "<<<<<<<<<Episode: 41\n",
      "[ 0.02430556 -0.06757813 -0.21757813 -0.8       ] -1.25995 {'score': 0}\n",
      "<<<<<<<<<Episode: 42\n",
      "[ 0.02430556 -0.06171875 -0.21171875  0.1       ] -1.1398328125000001 {'score': 0}\n",
      "<<<<<<<<<Episode: 43\n",
      "[ 0.02430556 -0.15742187 -0.30742188 -0.9       ] -1.3003796875 {'score': 0}\n",
      "<<<<<<<<<Episode: 44\n",
      "[ 0.         -0.00507813 -0.15507813 -0.9       ] -0.9687421875 {'score': 0}\n",
      "<<<<<<<<<Episode: 45\n",
      "[ 0.02430556 -0.07929688 -0.22929688 -0.9       ] -1.050184375 {'score': 0}\n",
      "<<<<<<<<<Episode: 46\n",
      "[ 0.       -0.003125 -0.153125 -0.2     ] 0.40616874999999975 {'score': 1}\n",
      "<<<<<<<<<Episode: 47\n",
      "[ 0.         -0.00117188 -0.15117188 -0.7       ] -0.8452953125 {'score': 0}\n",
      "<<<<<<<<<Episode: 48\n",
      "[ 0.02430556  0.153125    0.003125   -0.7       ] -1.0988171875000001 {'score': 0}\n",
      "<<<<<<<<<Episode: 49\n",
      "[ 0.24652778  0.2078125   0.0578125  -0.3       ] -0.640840625 {'score': 1}\n",
      "<<<<<<<<<Episode: 50\n",
      "[ 0.02430556  0.24492188  0.09492188 -0.2       ] -1.58104375 {'score': 0}\n",
      "<<<<<<<<<Episode: 51\n",
      "[ 0.02430556  0.184375    0.034375   -0.7       ] -1.2300671875 {'score': 0}\n",
      "<<<<<<<<<Episode: 52\n",
      "[ 0.24652778 -0.23164062 -0.38164063 -0.2       ] -0.5576375 {'score': 1}\n",
      "<<<<<<<<<Episode: 53\n",
      "[ 0.02430556 -0.09296875 -0.24296875 -0.1       ] -1.220021875 {'score': 1}\n",
      "<<<<<<<<<Episode: 54\n",
      "[ 0.       -0.003125 -0.153125 -0.3     ] 3.2459406249999967 {'score': 4}\n",
      "<<<<<<<<<Episode: 55\n",
      "[0.02430556 0.16484375 0.01484375 0.2       ] -1.132215625 {'score': 0}\n",
      "<<<<<<<<<Episode: 56\n",
      "[0.       0.153125 0.003125 0.5     ] -0.9859 {'score': 0}\n",
      "<<<<<<<<<Episode: 57\n",
      "[0.         0.15117188 0.00117188 0.6       ] -0.92898125 {'score': 0}\n",
      "<<<<<<<<<Episode: 58\n",
      "[ 0.         -0.00507813 -0.15507813 -0.4       ] -0.749015625 {'score': 0}\n",
      "<<<<<<<<<Episode: 59\n",
      "[ 0.        -0.0109375 -0.1609375 -0.9      ] -0.7502078125 {'score': 1}\n",
      "<<<<<<<<<Episode: 60\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.2806875 {'score': 2}\n",
      "<<<<<<<<<Episode: 61\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.020415625000000825 {'score': 3}\n",
      "<<<<<<<<<Episode: 62\n",
      "[ 0.02430556 -0.02460938 -0.17460938 -0.2       ] 5.176943749999995 {'score': 7}\n",
      "<<<<<<<<<Episode: 63\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.7717187499999999 {'score': 1}\n",
      "<<<<<<<<<Episode: 64\n",
      "[ 0.02430556 -0.35078125 -0.50078125 -0.9       ] 7.053160937499994 {'score': 12}\n",
      "<<<<<<<<<Episode: 65\n",
      "[ 0.02430556  0.15703125  0.00703125 -0.9       ] -1.022059375 {'score': 0}\n",
      "<<<<<<<<<Episode: 66\n",
      "[0.         0.16289063 0.01289063 0.8       ] -0.8500484375 {'score': 0}\n",
      "<<<<<<<<<Episode: 67\n",
      "[ 0.02430556 -0.41132812 -0.56132812 -0.9       ] -1.8160375000000002 {'score': 1}\n",
      "<<<<<<<<<Episode: 68\n",
      "[ 0.         -0.00507813 -0.15507813 -0.5       ] 1.4463718750000008 {'score': 7}\n",
      "<<<<<<<<<Episode: 69\n",
      "[ 0.         0.1609375  0.0109375 -0.9      ] 0.3189765625000005 {'score': 3}\n",
      "<<<<<<<<<Episode: 70\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.9429265625 {'score': 0}\n",
      "<<<<<<<<<Episode: 71\n",
      "[ 0.02430556 -0.065625   -0.215625    1.        ] -0.6357921875000001 {'score': 2}\n",
      "<<<<<<<<<Episode: 72\n",
      "[ 0.02430556 -0.11640625 -0.26640625  0.9       ] -0.8039562499999999 {'score': 2}\n",
      "<<<<<<<<<Episode: 73\n",
      "[ 0.02430556 -0.04804688 -0.19804688  1.        ] -0.5842296874999997 {'score': 2}\n",
      "<<<<<<<<<Episode: 74\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.765859375 {'score': 1}\n",
      "<<<<<<<<<Episode: 75\n",
      "[ 0.02430556 -0.12421875 -0.27421875  0.7       ] 1.5233890625000037 {'score': 7}\n",
      "<<<<<<<<<Episode: 76\n",
      "[ 0.          0.15507813  0.00507813 -0.9       ] 2.9539593750000037 {'score': 9}\n",
      "<<<<<<<<<Episode: 77\n",
      "[ 0.02430556 -0.52265625 -0.67265625 -0.7       ] 0.1510562500000021 {'score': 8}\n",
      "<<<<<<<<<Episode: 78\n",
      "[ 0.       -0.003125 -0.153125 -0.3     ] -0.16872187500000013 {'score': 2}\n",
      "<<<<<<<<<Episode: 79\n",
      "[ 0.02430556 -0.034375   -0.184375    1.        ] -0.30242656249999933 {'score': 3}\n",
      "<<<<<<<<<Episode: 80\n",
      "[ 0.26041667  0.07695312 -0.07304687  1.        ] 0.8508062499999982 {'score': 4}\n",
      "<<<<<<<<<Episode: 81\n",
      "[ 0.          0.15117188  0.00117188 -0.9       ] 8.847093749999981 {'score': 16}\n",
      "<<<<<<<<<Episode: 82\n",
      "[ 0.02430556 -0.00703125 -0.15703125 -0.3       ] -0.9129578125000002 {'score': 0}\n",
      "<<<<<<<<<Episode: 83\n",
      "[ 0.          0.15507813  0.00507813 -0.9       ] -0.09584687500000055 {'score': 2}\n",
      "<<<<<<<<<Episode: 84\n",
      "[ 0.26041667 -0.03046875 -0.18046875  1.        ] -0.5112812500000002 {'score': 2}\n",
      "<<<<<<<<<Episode: 85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-18ab30b6fde6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2498efdf8cd6>\u001b[0m in \u001b[0;36mchoose_action\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mactions_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#new reward\n",
    "score_record=[]\n",
    "reward_record=[]\n",
    "\n",
    "\n",
    "new_reward=0\n",
    "in_area=0\n",
    "is_pass=0\n",
    "is_crash=0\n",
    "\n",
    "score=0\n",
    "\n",
    "episode=500\n",
    "\n",
    "for i in range(episode):\n",
    "\n",
    "    print('<<<<<<<<<Episode: %s' % (i+1))\n",
    "    state = env.reset()  \n",
    "\n",
    "    episode_reward_sum = 0                                              \n",
    "\n",
    "    new_reward=0\n",
    "    while True:\n",
    "      \n",
    "        action = dqn.choose_action(state)                                        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "         \n",
    "        #env.render()\n",
    "        #time.sleep(1 / 300) \n",
    "        \n",
    "        #k=(abs(next_state[1])+abs(next_state[2]))/2\n",
    "        \n",
    "        if next_state[0]<0.2:\n",
    "           \n",
    "            if (next_state[1]>0 and next_state[2]<0):\n",
    "                if abs(next_state[1])>abs(next_state[2]):\n",
    "                    in_area=0.3*abs(next_state[2])\n",
    "                else:\n",
    "                    in_area=0.3*abs(next_state[1])\n",
    "            else:\n",
    "                if (next_state[1]<0 and next_state[2]<0):\n",
    "                    in_area=(-0.3)*abs(next_state[1])\n",
    "                else:\n",
    "                    in_area=(-0.3)*abs(next_state[2])\n",
    "\n",
    "                         \n",
    "        else:\n",
    "            in_area=0\n",
    "        \n",
    "        \n",
    "        \n",
    "        if done==True:\n",
    "            is_crash=-1\n",
    "        else:\n",
    "            is_crash=0\n",
    "        \n",
    "        if info['score']>score:\n",
    "            score=score+1\n",
    "            is_pass=1\n",
    "        else:\n",
    "            is_pass=0      \n",
    "      \n",
    "        new_reward=in_area+reward*0.0001+is_pass+is_crash\n",
    "                \n",
    "        dqn.store_transition(state, action, new_reward, next_state) \n",
    "        \n",
    "        episode_reward_sum = episode_reward_sum + new_reward                           \n",
    "\n",
    "        state = next_state                                               \n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:              \n",
    "            \n",
    "            dqn.learn()\n",
    "\n",
    "        if done:\n",
    "\n",
    "            print(next_state, episode_reward_sum,info)\n",
    "            score_record.append(info['score'])\n",
    "            reward_record.append(episode_reward_sum)\n",
    "            \n",
    "            break\n",
    "            \n",
    "    #env.close()         \n",
    "            \n",
    "env.close() \n",
    "\n",
    "dqn.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98225d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(score_record)),score_record)\n",
    "plt.title(\"Score Record\")\n",
    "plt.ylabel(\"Score\") \n",
    "plt.xlabel(\"Episode\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(reward_record)),reward_record,'g')\n",
    "plt.title(\"Reward Sum Record\") \n",
    "plt.ylabel(\"Reward Sum\") \n",
    "plt.xlabel(\"Episode\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score=0\n",
    "score_sum=0\n",
    "reward_sum=0\n",
    "\n",
    "for i in range(episode):\n",
    "    if max_score<score_record[i]:\n",
    "        max_score=score_record[i]\n",
    "    reward_sum=reward_sum+reward_record[i]\n",
    "    score_sum=score_sum+score_record[i]\n",
    "\n",
    "print(max_score)\n",
    "print(score_sum)\n",
    "print(reward_sum)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
