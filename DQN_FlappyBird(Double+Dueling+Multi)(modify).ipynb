{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b99a0de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                  \n",
    "import torch.nn as nn                          \n",
    "import torch.nn.functional as F  \n",
    "\n",
    "import numpy as np      \n",
    "\n",
    "import time\n",
    "import flappy_bird_gym\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665713b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32                                 \n",
    "LR = 0.001                                       \n",
    "EPSILON = 0.999                \n",
    "GAMMA = 0.9                                     \n",
    "TARGET_REPLACE_ITER = 100                       \n",
    "MEMORY_CAPACITY = 1000\n",
    "\n",
    "env = flappy_bird_gym.make(\"FlappyBird-v0\")         \n",
    "N_ACTIONS = env.action_space.n                 \n",
    "N_STATES = env.observation_space.shape[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1e8e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, n_hidden):                                                         \n",
    "\n",
    "        super(Net, self).__init__()                                             \n",
    "\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(n_states, n_hidden),                                                \n",
    "            nn.ReLU(),          \n",
    "            nn.Linear(n_hidden, n_hidden),                                                \n",
    "            nn.ReLU(),        \n",
    "        )\n",
    "        \n",
    "        self.advantage_layer = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden,n_actions ),                                                                                 \n",
    "        )\n",
    "        \n",
    "        self.value_layer = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden,1 ),                                                                                  \n",
    "        )\n",
    "                                   \n",
    "    def forward(self, x):                                                       \n",
    "        feature = self.feature_layer(x)           \n",
    "        value = self.value_layer(feature)\n",
    "        advantage = self.advantage_layer(feature)\n",
    "                                            \n",
    "        q = value + advantage - advantage.mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        return q              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc16f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, n_states, n_actions, n_hidden, \n",
    "                 batch_size, lr, epsilon, gamma, target_replace_iter, memory_capacity): \n",
    "                                                             \n",
    "        self.eval_net = Net(n_states, n_actions, n_hidden)\n",
    "        self.target_net = Net(n_states, n_actions, n_hidden)\n",
    "        \n",
    "        self.learn_step_counter = 0                                             \n",
    "        self.memory_counter = 0\n",
    "        \n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=LR)    \n",
    "        self.loss_func = nn.MSELoss()\n",
    "        \n",
    "        self.loss_record=[]\n",
    "\n",
    "    def choose_action(self, x):                                                 \n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)                            \n",
    "        if np.random.uniform() < EPSILON:                                       \n",
    "            actions_value = self.eval_net.forward(x)                            \n",
    "            action = torch.max(actions_value, 1)[1].data.numpy()                \n",
    "            action = action[0]                                                  \n",
    "        else:                                                                  \n",
    "            action = np.random.randint(0, N_ACTIONS)                        \n",
    "        return action                                                           \n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):                                    \n",
    "        transition = np.hstack((state, [action, reward], next_state))                                \n",
    "\n",
    "        index = self.memory_counter % MEMORY_CAPACITY                           \n",
    "        self.memory[index, :] = transition                                      \n",
    "        self.memory_counter =self.memory_counter + 1                                                \n",
    "\n",
    "    def learn(self):                                                            \n",
    "        #print(self.memory.shape)\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:                  \n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())         \n",
    "        self.learn_step_counter = self.learn_step_counter + 1                                            \n",
    "\n",
    "        sample_index = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)            \n",
    "        \n",
    "        \n",
    "        \n",
    "        b_memory = np.zeros((BATCH_SIZE, N_STATES * 2 + 2))\n",
    "        \n",
    "        N=3\n",
    "        for i in range(BATCH_SIZE):\n",
    "            r=0\n",
    "           \n",
    "            s=self.memory[(sample_index[i]), :N_STATES]\n",
    "            a=self.memory[(sample_index[i]), N_STATES:N_STATES+1]\n",
    "            for j in range(N):\n",
    "                r+=self.memory[(sample_index[i]+j)%MEMORY_CAPACITY, N_STATES+1:N_STATES+2]\n",
    "            s_=self.memory[(sample_index[i]+N)%MEMORY_CAPACITY, -N_STATES:]\n",
    "            t = np.hstack((s, a, r, s_))\n",
    "            b_memory[i, :] = t\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        #b_memory = self.memory[sample_index, :]        \n",
    "        \n",
    "        b_s = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        \n",
    "        b_a = torch.LongTensor(b_memory[:, N_STATES:N_STATES+1].astype(int))\n",
    "       \n",
    "        b_r = torch.FloatTensor(b_memory[:, N_STATES+1:N_STATES+2])\n",
    "       \n",
    "        b_s_ = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        \n",
    "        q_eval = self.eval_net(b_s).gather(1, b_a)\n",
    "\n",
    "        q_next = self.target_net(b_s_).gather(\n",
    "        1, self.eval_net(b_s_).argmax(dim=1, keepdim=True)\n",
    "        ).detach()\n",
    "  \n",
    "        q_target = b_r + (GAMMA) * q_next.max(1)[0].view(BATCH_SIZE, 1)\n",
    "\n",
    "        loss = self.loss_func(q_eval, q_target)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.loss_record.append(loss)\n",
    "        \n",
    "        self.optimizer.zero_grad()                                      \n",
    "        loss.backward()                                                 \n",
    "        self.optimizer.step()\n",
    "        \n",
    "\n",
    "    def plot_loss(self):\n",
    "        \n",
    "        plt.plot(np.arange(len(self.loss_record)),self.loss_record,'r')\n",
    "        plt.title(\"Loss Record\") # title\n",
    "        plt.ylabel(\"Loss\") # y label\n",
    "        plt.xlabel(\"Step\") # x label\n",
    "        plt.show()\n",
    "        \n",
    "    def save_params(self):\n",
    "        #print(self.eval_net.state_dict())\n",
    "        torch.save(self.eval_net.state_dict(),'params.pkl')\n",
    "    \n",
    "    def load_params(self):\n",
    "        self.eval_net.load_state_dict(torch.load('params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43478fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(N_STATES,N_ACTIONS,128,BATCH_SIZE,LR,EPSILON,GAMMA,TARGET_REPLACE_ITER,MEMORY_CAPACITY)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bc4f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<Episode: 1\n",
      "[ 0.02430556 -0.45429687 -0.60429687 -0.5       ] -2.817371875 {'score': 0}\n",
      "<<<<<<<<<Episode: 2\n",
      "[ 0.02430556 -0.48945312 -0.63945312 -0.4       ] -2.9328015625 {'score': 0}\n",
      "<<<<<<<<<Episode: 3\n",
      "[ 0.02430556 -0.40742187 -0.55742187 -0.5       ] -2.6204968749999997 {'score': 0}\n",
      "<<<<<<<<<Episode: 4\n",
      "[ 0.02430556 -0.31367187 -0.46367188 -0.6       ] -2.2701062499999995 {'score': 0}\n",
      "<<<<<<<<<Episode: 5\n",
      "[ 0.02430556 -0.57148438 -0.72148437 -0.3       ] -3.2562390625000006 {'score': 0}\n",
      "<<<<<<<<<Episode: 6\n",
      "[ 0.02430556 -0.48164062 -0.63164062 -0.5       ] -2.932215625 {'score': 0}\n",
      "<<<<<<<<<Episode: 7\n",
      "[ 0.02430556 -0.34492187 -0.49492188 -0.7       ] -2.4558484375000003 {'score': 0}\n",
      "<<<<<<<<<Episode: 8\n",
      "[ 0.02430556 -0.50898438 -0.65898437 -0.4       ] -3.0148328125 {'score': 0}\n",
      "<<<<<<<<<Episode: 9\n",
      "[ 0.02430556 -0.4015625  -0.5515625  -0.5       ] -2.5958874999999995 {'score': 0}\n",
      "<<<<<<<<<Episode: 10\n",
      "[ 0.02430556 -0.47578125 -0.62578125 -0.5       ] -2.9076062500000006 {'score': 0}\n",
      "<<<<<<<<<Episode: 11\n",
      "[ 0.02430556 -0.46210937 -0.61210937 -0.5       ] -2.850184375 {'score': 0}\n",
      "<<<<<<<<<Episode: 12\n",
      "[ 0.02430556 -0.50703125 -0.65703125 -0.4       ] -3.0066296875000003 {'score': 0}\n",
      "<<<<<<<<<Episode: 13\n",
      "[ 0.02430556 -0.46015625 -0.61015625 -0.5       ] -2.84198125 {'score': 0}\n",
      "<<<<<<<<<Episode: 14\n",
      "[ 0.02430556 -0.45039062 -0.60039062 -0.5       ] -2.800965625 {'score': 0}\n",
      "<<<<<<<<<Episode: 15\n",
      "[ 0.02430556 -0.52265625 -0.67265625 -0.4       ] -3.0722546875 {'score': 0}\n",
      "<<<<<<<<<Episode: 16\n",
      "[ 0.02430556 -0.51679688 -0.66679687 -0.4       ] -3.0476453125 {'score': 0}\n",
      "<<<<<<<<<Episode: 17\n",
      "[0.45486111 0.33375    0.18375    1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 18\n",
      "[ 0.02430556 -0.0734375  -0.2234375   1.        ] -1.7585828124999994 {'score': 0}\n",
      "<<<<<<<<<Episode: 19\n",
      "[ 0.02430556 -0.25117187 -0.40117188  1.        ] -2.4997937500000003 {'score': 0}\n",
      "<<<<<<<<<Episode: 20\n",
      "[ 0.02430556 -0.29804687 -0.44804688  1.        ] -2.6322156249999997 {'score': 0}\n",
      "<<<<<<<<<Episode: 21\n",
      "[ 0.02430556 -0.24921875 -0.39921875  1.        ] -2.4968640624999994 {'score': 0}\n",
      "<<<<<<<<<Episode: 22\n",
      "[ 0.02430556 -0.34492187 -0.49492188  1.        ] -2.8788953125000005 {'score': 0}\n",
      "<<<<<<<<<Episode: 23\n",
      "[ 0.02430556 -0.19257812 -0.34257813  1.        ] -2.257215625 {'score': 0}\n",
      "<<<<<<<<<Episode: 24\n",
      "[ 0.02430556 -0.1203125  -0.2703125   1.        ] -1.9355359375000003 {'score': 0}\n",
      "<<<<<<<<<Episode: 25\n",
      "[ 0.02430556 -0.0890625  -0.2390625   1.        ] -1.818934375 {'score': 0}\n",
      "<<<<<<<<<Episode: 26\n",
      "[ 0.02430556 -0.23359375 -0.38359375  1.        ] -2.3990125 {'score': 0}\n",
      "<<<<<<<<<Episode: 27\n",
      "[ 0.02430556 -0.11054688 -0.26054688  1.        ] -1.9126843750000002 {'score': 0}\n",
      "<<<<<<<<<Episode: 28\n",
      "[ 0.02430556 -0.01875    -0.16875     1.        ] -1.5288953125 {'score': 0}\n",
      "<<<<<<<<<Episode: 29\n",
      "[ 0.02430556 -0.1359375  -0.2859375  -0.5       ] -1.52948125 {'score': 0}\n",
      "<<<<<<<<<Episode: 30\n",
      "[ 0.02430556 -0.01289063 -0.16289063 -0.1       ] 0.19599374999999974 {'score': 1}\n",
      "<<<<<<<<<Episode: 31\n",
      "[ 0.02430556 -0.26679687 -0.41679688  0.2       ] -2.038075 {'score': 0}\n",
      "<<<<<<<<<Episode: 32\n",
      "[ 0.02430556 -0.49140625 -0.64140625  0.8       ] -3.2556531250000003 {'score': 0}\n",
      "<<<<<<<<<Episode: 33\n",
      "[ 0.02430556 -0.41132812 -0.56132812  0.5       ] -2.7822156249999996 {'score': 0}\n",
      "<<<<<<<<<Episode: 34\n",
      "[ 0.02430556 -0.32148437 -0.47148438  0.2       ] -2.2677625 {'score': 0}\n",
      "<<<<<<<<<Episode: 35\n",
      "[ 0.02430556 -0.2765625  -0.4265625  -0.9       ] -1.966590625 {'score': 0}\n",
      "<<<<<<<<<Episode: 36\n",
      "[0.02430556 0.35039063 0.20039062 1.        ] -1.378309375 {'score': 0}\n",
      "<<<<<<<<<Episode: 37\n",
      "[ 0.02430556 -0.36054687 -0.51054687 -0.7       ] -2.2343640625 {'score': 0}\n",
      "<<<<<<<<<Episode: 38\n",
      "[ 0.45486111  0.3884375   0.2384375  -0.9       ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 39\n",
      "[ 0.02430556  0.2         0.05       -0.1       ] -1.36073125 {'score': 0}\n",
      "<<<<<<<<<Episode: 40\n",
      "[0.02430556 0.26445313 0.11445313 0.9       ] -1.2306531250000001 {'score': 0}\n",
      "<<<<<<<<<Episode: 41\n",
      "[ 0.02430556 -0.1125     -0.2625     -0.6       ] -1.214246875 {'score': 0}\n",
      "<<<<<<<<<Episode: 42\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.3486531249999999 {'score': 1}\n",
      "<<<<<<<<<Episode: 43\n",
      "[0.       0.153125 0.003125 1.      ] -0.69143125 {'score': 1}\n",
      "<<<<<<<<<Episode: 44\n",
      "[ 0.        0.153125  0.003125 -0.9     ] 0.5192078124999999 {'score': 2}\n",
      "<<<<<<<<<Episode: 45\n",
      "[ 0.         -0.00117188 -0.15117188 -0.2       ] 1.0653734374999995 {'score': 3}\n",
      "<<<<<<<<<Episode: 46\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.7808421874999999 {'score': 0}\n",
      "<<<<<<<<<Episode: 47\n",
      "[ 0.02430556 -0.065625   -0.215625    0.8       ] -0.9740453125000003 {'score': 1}\n",
      "<<<<<<<<<Episode: 48\n",
      "[ 0.       -0.003125 -0.153125 -0.3     ] -0.03203125000000007 {'score': 3}\n",
      "<<<<<<<<<Episode: 49\n",
      "[ 0.          0.16289063  0.01289063 -0.9       ] 4.230514062500002 {'score': 6}\n",
      "<<<<<<<<<Episode: 50\n",
      "[ 0.02430556 -0.28828125 -0.43828125 -0.8       ] -0.6449656249999993 {'score': 3}\n",
      "<<<<<<<<<Episode: 51\n",
      "[ 0.       -0.003125 -0.153125 -0.4     ] -0.4096281250000001 {'score': 2}\n",
      "<<<<<<<<<Episode: 52\n",
      "[ 0.         -0.00507813 -0.15507813 -0.4       ] 1.0156281249999999 {'score': 4}\n",
      "<<<<<<<<<Episode: 53\n",
      "[ 0.          0.15703125  0.00703125 -0.9       ] -0.9247109375000001 {'score': 0}\n",
      "<<<<<<<<<Episode: 54\n",
      "[0.45486111 0.32398438 0.17398438 1.        ] -0.9968 {'score': 0}\n",
      "<<<<<<<<<Episode: 55\n",
      "[ 0.02430556 -0.04609375 -0.19609375  1.        ] 0.049153124999998354 {'score': 5}\n",
      "<<<<<<<<<Episode: 56\n",
      "[ 0.          0.15117188  0.00117188 -0.9       ] 0.7222062500000002 {'score': 3}\n",
      "<<<<<<<<<Episode: 57\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.08879843750000038 {'score': 2}\n",
      "<<<<<<<<<Episode: 58\n",
      "[ 0.         -0.00507813 -0.15507813 -0.7       ] -0.6289015625000001 {'score': 1}\n",
      "<<<<<<<<<Episode: 59\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] 1.447462499999998 {'score': 6}\n",
      "<<<<<<<<<Episode: 60\n",
      "[ 0.          0.15117188  0.00117188 -0.9       ] 0.7682296875000001 {'score': 5}\n",
      "<<<<<<<<<Episode: 61\n",
      "[ 0.02430556 -0.05976563 -0.20976563  0.9       ] -1.1630687499999999 {'score': 1}\n",
      "<<<<<<<<<Episode: 62\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.9295359375 {'score': 0}\n",
      "<<<<<<<<<Episode: 63\n",
      "[ 0.02430556 -0.01289063 -0.16289063  0.        ] -0.949871875 {'score': 0}\n",
      "<<<<<<<<<Episode: 64\n",
      "[ 0.02430556 -0.00703125 -0.15703125  1.        ] -0.8791234375000002 {'score': 1}\n",
      "<<<<<<<<<Episode: 65\n",
      "[ 0.        0.153125  0.003125 -0.9     ] -0.2073078125000003 {'score': 1}\n",
      "<<<<<<<<<Episode: 66\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.31542187500000096 {'score': 2}\n",
      "<<<<<<<<<Episode: 67\n",
      "[ 0.        -0.0109375 -0.1609375 -0.7      ] -0.7477078125000001 {'score': 1}\n",
      "<<<<<<<<<Episode: 68\n",
      "[ 0.23263889  0.21367188  0.06367188 -0.9       ] -0.5932796875000003 {'score': 1}\n",
      "<<<<<<<<<Episode: 69\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.3096640625000002 {'score': 1}\n",
      "<<<<<<<<<Episode: 70\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] 0.5081703124999997 {'score': 4}\n",
      "<<<<<<<<<Episode: 71\n",
      "[ 0.02430556  0.153125    0.003125   -0.8       ] 5.003992187499998 {'score': 9}\n",
      "<<<<<<<<<Episode: 72\n",
      "[ 0.02430556  0.184375    0.034375   -0.9       ] 0.015034374999999489 {'score': 3}\n",
      "<<<<<<<<<Episode: 73\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.8034453125 {'score': 1}\n",
      "<<<<<<<<<Episode: 74\n",
      "[ 0.02430556  0.17460938  0.02460938 -0.9       ] -0.9195531250000001 {'score': 1}\n",
      "<<<<<<<<<Episode: 75\n",
      "[ 0.        0.153125  0.003125 -0.9     ] -0.8551562500000001 {'score': 1}\n",
      "<<<<<<<<<Episode: 76\n",
      "[ 0.23263889  0.14335938 -0.00664062  1.        ] -0.5352718749999998 {'score': 1}\n",
      "<<<<<<<<<Episode: 77\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] 2.1240031250000015 {'score': 8}\n",
      "<<<<<<<<<Episode: 78\n",
      "[ 0.02430556 -0.0578125  -0.2078125   0.7       ] 1.1712406250000038 {'score': 7}\n",
      "<<<<<<<<<Episode: 79\n",
      "[ 0.02430556 -0.0109375  -0.1609375   1.        ] -0.035824999999999996 {'score': 3}\n",
      "<<<<<<<<<Episode: 80\n",
      "[ 0.02430556 -0.00117188 -0.15117188 -0.4       ] 1.3515078125000008 {'score': 6}\n",
      "<<<<<<<<<Episode: 81\n",
      "[ 0.02430556 -0.02070313 -0.17070313 -0.3       ] 0.3207765625000003 {'score': 3}\n",
      "<<<<<<<<<Episode: 82\n",
      "[ 0.02430556  0.15507813  0.00507813 -0.9       ] -0.5281468750000003 {'score': 1}\n",
      "<<<<<<<<<Episode: 83\n",
      "[ 0.02430556 -0.02070313 -0.17070313  0.2       ] 0.6234046874999981 {'score': 4}\n",
      "<<<<<<<<<Episode: 84\n",
      "[ 0.       -0.003125 -0.153125 -0.2     ] 0.06339062499999981 {'score': 3}\n",
      "<<<<<<<<<Episode: 85\n",
      "[ 0.        0.153125  0.003125 -0.9     ] 0.701024999999998 {'score': 5}\n",
      "<<<<<<<<<Episode: 86\n",
      "[ 0.       -0.003125 -0.153125 -0.3     ] 8.670226562499984 {'score': 14}\n",
      "<<<<<<<<<Episode: 87\n",
      "[ 0.       -0.003125 -0.153125 -0.3     ] -0.2438078125000004 {'score': 2}\n",
      "<<<<<<<<<Episode: 88\n",
      "[ 0.         -0.00898438 -0.15898438 -0.5       ] -0.8812515625 {'score': 0}\n",
      "<<<<<<<<<Episode: 89\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.5726656250000002 {'score': 2}\n",
      "<<<<<<<<<Episode: 90\n",
      "[0.02430556 0.17460938 0.02460938 0.        ] -0.8310765625000003 {'score': 1}\n",
      "<<<<<<<<<Episode: 91\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.18070781250000068 {'score': 2}\n",
      "<<<<<<<<<Episode: 92\n",
      "[ 0.02430556 -0.01289063 -0.16289063  0.9       ] -0.8227062500000002 {'score': 2}\n",
      "<<<<<<<<<Episode: 93\n",
      "[ 0.02430556 -0.07734375 -0.22734375 -0.9       ] -1.0822093750000001 {'score': 1}\n",
      "<<<<<<<<<Episode: 94\n",
      "[ 0.26041667 -0.17890625 -0.32890625 -0.4       ] -0.38565625000000014 {'score': 2}\n",
      "<<<<<<<<<Episode: 95\n",
      "[ 0.          0.15117188  0.00117188 -0.9       ] 2.691026562500004 {'score': 10}\n",
      "<<<<<<<<<Episode: 96\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] 0.01066093750000019 {'score': 3}\n",
      "<<<<<<<<<Episode: 97\n",
      "[ 0.        -0.0109375 -0.1609375 -0.6      ] 3.5226578124999977 {'score': 12}\n",
      "<<<<<<<<<Episode: 98\n",
      "[ 0.         -0.00117188 -0.15117188 -0.4       ] -0.8766828124999999 {'score': 1}\n",
      "<<<<<<<<<Episode: 99\n",
      "[ 0.02430556 -0.01679688 -0.16679688 -0.9       ] -0.9884593750000001 {'score': 1}\n",
      "<<<<<<<<<Episode: 100\n",
      "[ 0.02430556 -0.09101563 -0.24101563 -0.7       ] -0.12312968749999986 {'score': 3}\n",
      "<<<<<<<<<Episode: 101\n",
      "[ 0.02430556 -0.02265625 -0.17265625  0.8       ] 2.624165625000005 {'score': 11}\n",
      "<<<<<<<<<Episode: 102\n",
      "[ 0.          0.15507813  0.00507813 -0.9       ] -0.5345093750000003 {'score': 1}\n",
      "<<<<<<<<<Episode: 103\n",
      "[ 0.02430556 -0.18671875 -0.33671875 -0.8       ] 0.7867484375000029 {'score': 7}\n",
      "<<<<<<<<<Episode: 104\n",
      "[ 0.02430556 -0.01484375 -0.16484375 -0.1       ] 3.491135937500001 {'score': 13}\n",
      "<<<<<<<<<Episode: 105\n",
      "[ 0.         -0.00117188 -0.15117188 -0.1       ] -0.8098046875 {'score': 1}\n",
      "<<<<<<<<<Episode: 106\n",
      "[ 0.02430556 -0.3390625  -0.4890625  -0.9       ] -1.2593796874999998 {'score': 3}\n",
      "<<<<<<<<<Episode: 107\n",
      "[ 0.02430556 -0.01289063 -0.16289063 -0.2       ] -0.18872187500000048 {'score': 2}\n",
      "<<<<<<<<<Episode: 108\n",
      "[ 0.02430556 -0.01289063 -0.16289063  1.        ] -0.7131359375 {'score': 2}\n",
      "<<<<<<<<<Episode: 109\n",
      "[ 0.02430556 -0.34492187 -0.49492188 -0.9       ] -1.75744375 {'score': 1}\n",
      "<<<<<<<<<Episode: 110\n",
      "[ 0.02430556 -0.13007812 -0.28007813  0.4       ] -0.31890000000000107 {'score': 4}\n",
      "<<<<<<<<<Episode: 111\n",
      "[ 0.02430556 -0.18085937 -0.33085938  0.6       ] -1.40770625 {'score': 2}\n",
      "<<<<<<<<<Episode: 112\n",
      "[ 0.26041667 -0.034375   -0.184375    1.        ] 1.423887499999999 {'score': 8}\n",
      "<<<<<<<<<Episode: 113\n",
      "[ 0.02430556 -0.1046875  -0.2546875   0.9       ] -0.16983749999999986 {'score': 4}\n",
      "<<<<<<<<<Episode: 114\n",
      "[ 0.02430556 -0.00507813 -0.15507813 -0.4       ] 16.22785624999997 {'score': 22}\n",
      "<<<<<<<<<Episode: 115\n",
      "[ 0.02430556 -0.03828125 -0.18828125  1.        ] 2.957446875000002 {'score': 11}\n",
      "<<<<<<<<<Episode: 116\n",
      "[ 0.          0.15117188  0.00117188 -0.9       ] 0.6845734375000014 {'score': 4}\n",
      "<<<<<<<<<Episode: 117\n",
      "[ 0.02430556  0.15117188  0.00117188 -0.1       ] 1.2114359375000001 {'score': 7}\n",
      "<<<<<<<<<Episode: 118\n",
      "[ 0.34375     0.49390625  0.34390625 -0.9       ] -0.996 {'score': 0}\n",
      "<<<<<<<<<Episode: 119\n",
      "[0.32986111 0.568125   0.418125   1.        ] -0.9959 {'score': 0}\n",
      "<<<<<<<<<Episode: 120\n",
      "[0.19097222 0.37867188 0.22867188 1.        ] -1.132103125 {'score': 0}\n",
      "<<<<<<<<<Episode: 121\n",
      "[ 0.          0.16679688  0.01679688 -0.9       ] -0.7244062500000001 {'score': 0}\n",
      "<<<<<<<<<Episode: 122\n",
      "[ 0.02430556 -0.0890625  -0.2390625   0.2       ] 1.0764359375000017 {'score': 7}\n",
      "<<<<<<<<<Episode: 123\n",
      "[ 0.         -0.00117188 -0.15117188 -0.1       ] 1.6451437500000003 {'score': 9}\n",
      "<<<<<<<<<Episode: 124\n",
      "[ 0.02430556 -0.02070313 -0.17070313  1.        ] -0.6074328125000006 {'score': 2}\n",
      "<<<<<<<<<Episode: 125\n",
      "[ 0.02430556 -0.07148438 -0.22148438  1.        ] 7.989592187499983 {'score': 24}\n",
      "<<<<<<<<<Episode: 126\n",
      "[ 0.23263889 -0.08515625 -0.23515625  1.        ] 3.9801828124999936 {'score': 13}\n",
      "<<<<<<<<<Episode: 127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-18ab30b6fde6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory_counter\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mMEMORY_CAPACITY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-cb8f50c3e470>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dqn\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dqn\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dqn\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dqn\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#new reward\n",
    "score_record=[]\n",
    "reward_record=[]\n",
    "\n",
    "\n",
    "new_reward=0\n",
    "in_area=0\n",
    "is_pass=0\n",
    "is_crash=0\n",
    "\n",
    "score=0\n",
    "\n",
    "episode=500\n",
    "\n",
    "for i in range(episode):\n",
    "\n",
    "    print('<<<<<<<<<Episode: %s' % (i+1))\n",
    "    state = env.reset()  \n",
    "\n",
    "    episode_reward_sum = 0                                              \n",
    "\n",
    "    new_reward=0\n",
    "    while True:\n",
    "      \n",
    "        action = dqn.choose_action(state)                                        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "         \n",
    "        #env.render()\n",
    "        #time.sleep(1 / 300) \n",
    "        \n",
    "        #k=(abs(next_state[1])+abs(next_state[2]))/2\n",
    "        \n",
    "        if next_state[0]<0.2:\n",
    "           \n",
    "            if (next_state[1]>0 and next_state[2]<0):\n",
    "                if abs(next_state[1])>abs(next_state[2]):\n",
    "                    in_area=0.3*abs(next_state[2])\n",
    "                else:\n",
    "                    in_area=0.3*abs(next_state[1])\n",
    "            else:\n",
    "                if (next_state[1]<0 and next_state[2]<0):\n",
    "                    in_area=(-0.3)*abs(next_state[1])\n",
    "                else:\n",
    "                    in_area=(-0.3)*abs(next_state[2])\n",
    "\n",
    "                         \n",
    "        else:\n",
    "            in_area=0\n",
    "        \n",
    "        \n",
    "        \n",
    "        if done==True:\n",
    "            is_crash=-1\n",
    "        else:\n",
    "            is_crash=0\n",
    "        \n",
    "        if info['score']>score:\n",
    "            score=score+1\n",
    "            is_pass=1\n",
    "        else:\n",
    "            is_pass=0      \n",
    "      \n",
    "        new_reward=in_area+reward*0.0001+is_pass+is_crash\n",
    "                \n",
    "        dqn.store_transition(state, action, new_reward, next_state) \n",
    "        \n",
    "        episode_reward_sum = episode_reward_sum + new_reward                           \n",
    "\n",
    "        state = next_state                                               \n",
    "\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:              \n",
    "            \n",
    "            dqn.learn()\n",
    "\n",
    "        if done:\n",
    "\n",
    "            print(next_state, episode_reward_sum,info)\n",
    "            score_record.append(info['score'])\n",
    "            reward_record.append(episode_reward_sum)\n",
    "            \n",
    "            break\n",
    "            \n",
    "    #env.close()         \n",
    "            \n",
    "env.close() \n",
    "\n",
    "dqn.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5ad32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(score_record)),score_record)\n",
    "plt.title(\"Score Record\")\n",
    "plt.ylabel(\"Score\") \n",
    "plt.xlabel(\"Episode\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3dff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(reward_record)),reward_record,'g')\n",
    "plt.title(\"Reward Sum Record\") \n",
    "plt.ylabel(\"Reward Sum\") \n",
    "plt.xlabel(\"Episode\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8382da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score=0\n",
    "score_sum=0\n",
    "reward_sum=0\n",
    "\n",
    "for i in range(episode):\n",
    "    if max_score<score_record[i]:\n",
    "        max_score=score_record[i]\n",
    "    reward_sum=reward_sum+reward_record[i]\n",
    "    score_sum=score_sum+score_record[i]\n",
    "\n",
    "print(max_score)\n",
    "print(score_sum)\n",
    "print(reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430eb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
